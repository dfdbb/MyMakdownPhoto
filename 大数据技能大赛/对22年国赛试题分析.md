# 对22年大数据国赛卷分析

- Scala 作为整个项目的基础开发语言
- 基于大数据平台综合利用 Hive、Spark、Flink、Vue.js 等技术
- 数据进行处理、分析及可 视化呈现

## 模块 A：大数据平台搭建（容器环境）15分

环境为 一主二从 (master ,slave1,slave2) 
(master,salve1,salve2均为datanode)
- [ ] 基本docker命令应用
- [ ] ssh 工具的使用
- [ ] 基本linux命令的使用

### 任务一 Hadoop 完全分布式安装配置
前置环境配置：
> hadoop-2.7.7.tar.gz  jdk-8u212-linux-x64.tar.gz

- [ ] tar，mv，cp,scp，vim使用
- [ ] 配置jdk 
- [ ] 配置hosts
- [ ] hadoop基本配置
- [ ] hadoop集群基本使用，启动等

任务二：Sqoop 安装配置
> 主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql...)间进行数据的传递.
> sqoop-1.4.2.bin__hadoop-2.0.0-alpha.tar.gz、mysql-connector-java-5.1.47.jar

难点：
- [ ]  完善其他 Sqoop 相关配置，设置 Sqoop 环境变量，并使环境变量生效，执行 命令 sqoop version
-  [ ] 测试 Sqoop 连接容器 master 节点的 MySQL 数据库是否成功并展示所有的 database

### 任务三：Hive 安装配置
>apache-hive-2.3.4-bin.tar.gz、mysql-connector-java-5.1.47.jar

- [ ] 设置 Hive 环境变量，并使环境变量生效

- [ ] 完成相关配置并添加所依赖的包，将 MySQL 数据库作为 Hive 元数据库。初 始化 Hive 元数据，并通过 schematool 相关命令执行初始化


## 模块 B：离线数据处理（25 分）

### 任务一：数据抽取

- [ ] Hive的使用
- [ ] Sqoop 脚本,sqoop的使用
> 提示：可通过 sqoop 将 mysql 的数据先加 载到 hdfs，然后再通过 hive 中 load data inpath 的方式为将数据加载到分区 表中，同时 hive 表中默认的分隔符为\t

### 任务二：数据清洗
- [ ] Hive SQL

### 任务三：指标计算
- [ ] 编写 Scala 代码
- [ ] 使用 Spark
- [ ] 根据 dwd 层的 fact_change_record 表统计
- [ ] 计算结 果存入 MySQL 数据库
- [ ] MySQL 命令行（查询等基本操作)
- [ ] 编写 Scala 代码,剔除脏数据,联合主键去重

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTg5Nzc4MDk5XX0=
-->